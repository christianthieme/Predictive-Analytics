# apply imputation of numeric variables
df3 <- predict(pp, df3)
# remove rows without a brand code
df3b <- df3 %>%
filter(!is.na(brand_code))
# predict class using multinomial logistic regression
pc <- train(brand_code ~ .,
data = df3b,
method = 'multinom',
trControl = trainControl(method = 'cv', number = 10),
trace = F)
df3 <- df %>%
filter(!is.na(ph)) %>%
mutate(brand_code = factor(brand_code))
# remove near zero variance variables
isNZV <- nearZeroVar(df3)
df3 <- df3[, -isNZV]
# imputing numeric variables
pp <- df3 %>%
as.data.frame() %>%
preProcess(method = c('knnImpute'))
# apply imputation of numeric variables
df3 <- predict(pp, df3)
df3 %>%
filter(!is.na(brand_code))
df <- df %>%
mutate(
total_hyd_pressure = hyd_pressure1 + hyd_pressure2 + hyd_pressure3 + hyd_pressure4,
total_balling = balling + balling_lvl
) %>%
select(-hyd_pressure1, -hyd_pressure2, -hyd_pressure3, -hyd_pressure4, -balling, -balling_lvl)
df3 <- df %>%
filter(!is.na(ph)) %>%
mutate(brand_code = factor(brand_code))
# remove near zero variance variables
isNZV <- nearZeroVar(df3)
df3 <- df3[, -isNZV]
# imputing numeric variables
pp <- df3 %>%
as.data.frame() %>%
preProcess(method = c('knnImpute'))
predict(pp, df3)
df3 %>%
as.data.frame() %>%
preProcess(method = c('knnImpute'))
df %>%
filter(!is.na(ph)) %>%
mutate(brand_code = factor(brand_code))
isNZV <- nearZeroVar(df3)
df3[, -isNZV]
# remove rows without a response variable
df3 <- df %>%
filter(!is.na(ph)) %>%
mutate(brand_code = factor(brand_code))
# remove near zero variance variables
#isNZV <- nearZeroVar(df3)
#df3 <- df3[, -isNZV]
# imputing numeric variables
pp <- df3 %>%
as.data.frame() %>%
preProcess(method = c('knnImpute'))
# apply imputation of numeric variables
df3 <- predict(pp, df3)
# remove rows without a brand code
df3b <- df3 %>%
filter(!is.na(brand_code))
# predict class using multinomial logistic regression
pc <- train(brand_code ~ .,
data = df3b,
method = 'multinom',
trControl = trainControl(method = 'cv', number = 10),
trace = F)
knitr::opts_chunk$set(echo = TRUE, warning=F, message=F)
options(scipen = 9)
set.seed(101)
library(fpp2)
library(ggplot2)
library(tidyr)
library(dplyr)
library(stringr)
library(broom)
library(seasonal)
library(imputeTS)
library(tidymodels)
library(mice)
library(inspectdf)
library(vip)
library(lubridate)
library(corrplot)
library(caret)
library(fpp3)
library(randomForest)
library(Cubist)
library(knitr)
data_raw <- readr::read_csv('https://raw.githubusercontent.com/christianthieme/Predictive-Analytics/main/Project2/data/StudentData%20-%20TO%20MODEL.csv')
data_raw$obs_type <- "train"
eval_raw <- readr::read_csv('https://raw.githubusercontent.com/christianthieme/Predictive-Analytics/main/Project2/data/StudentEvaluation-%20TO%20PREDICT.csv')
eval_raw$obs_type <- "test"
combined <- data_raw %>%
rbind(eval_raw)
# convert column names to all lowercase
names(combined) <- lapply(names(combined), tolower)
# convert column name spaces to underscore
names(combined) <- str_replace_all(names(combined), ' ', '_')
df <- combined %>%
filter(obs_type == 'train') %>%
select(-obs_type)
eval <- combined %>%
filter(obs_type == 'test') %>%
select(-obs_type)
glimpse(combined)
df %>%
visdat::vis_miss(sort_miss = TRUE)
df %>%
select(ph) %>%
ggplot() +
aes(x = ph) +
geom_histogram(fill = "lightsalmon2", color = "black") +
labs(title = "Histogram of pH", y = "Count") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.45),
#   plot.margin = ggplot2::margin(10, 20, 10, 10),
panel.grid.major.y =  element_line(color = "grey", linetype = "dashed"),
panel.grid.major.x = element_blank(),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank()
)
df %>%
ggplot() +
aes(x = ph) +
geom_histogram(fill = "lightsalmon2", color = "black") +
labs(title = "Distribution of pH by Brand") +
facet_wrap(~brand_code, scales = 'free_y') +
theme(
plot.title = element_text(hjust = 0.45),
#   plot.margin = ggplot2::margin(10, 20, 10, 10),
panel.grid.major.y =  element_line(color = "grey", linetype = "dashed"),
panel.grid.major.x = element_blank(),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank()
)
df %>%
select(-ph) %>%
inspectdf::inspect_num() %>%
show_plot() +
labs(title = 'Distribution of Numeric Columns in Training Data')
df %>%
dplyr::select(brand_code, ph) %>%
ggplot() +
aes(y = ph) +
geom_boxplot(color = 'steelblue', outlier.color = 'firebrick', outlier.alpha = 0.35)+
labs(title = 'Brand Code Box Plots') +
facet_grid(~brand_code) +
#  theme_minimal() +
theme(
plot.title = element_text(hjust = 0.45),
panel.grid.major.y =  element_line(color = "grey", linetype = "dashed"),
panel.grid.major.x = element_blank(),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank(),
# panel.background = element_blank(),
axis.ticks.x = element_line(color = "grey")
)
#theme(axis.ticks.x = element_blank())
df %>%
select(-brand_code) %>%
gather(variable, value, -ph) %>%
ggplot(aes(x = value, y = ph)) +
geom_point(color = 'steelblue', alpha = 0.35) +
facet_wrap(~variable, scales = "free_x") +
labs(title = "pH Relationship with Independent Variables", x = element_blank())+
theme(
plot.title = element_text(hjust = 0.45),
panel.grid.major.y =  element_line(color = "grey", linetype = "dashed"),
panel.grid.major.x = element_blank(),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank(),
axis.ticks.x = element_line(color = "grey")
)
numeric_values <- df %>%
dplyr::select_if(is.numeric)
numeric_values <- numeric_values[complete.cases(numeric_values),] %>%
data.frame()
train_cor <- cor(numeric_values)
corrplot::corrplot.mixed(train_cor, tl.col = 'black', tl.pos = 'lt')
df <- df %>%
mutate(
total_hyd_pressure = hyd_pressure1 + hyd_pressure2 + hyd_pressure3 + hyd_pressure4,
total_balling = balling + balling_lvl
) %>%
select(-hyd_pressure1, -hyd_pressure2, -hyd_pressure3, -hyd_pressure4, -balling, -balling_lvl)
# remove rows without a response variable
df3 <- df %>%
filter(!is.na(ph)) %>%
mutate(brand_code = factor(brand_code))
# remove near zero variance variables
#isNZV <- nearZeroVar(df3)
#df3 <- df3[, -isNZV]
# imputing numeric variables
pp <- df3 %>%
as.data.frame() %>%
preProcess(method = c('knnImpute'))
# apply imputation of numeric variables
df3 <- predict(pp, df3)
# remove rows without a brand code
df3b <- df3 %>%
filter(!is.na(brand_code))
# predict class using multinomial logistic regression
pc <- train(brand_code ~ .,
data = df3b,
method = 'multinom',
trControl = trainControl(method = 'cv', number = 10),
trace = F)
# create new field with predictions of brand code
df3$brand2 <- predict(pc, df3)
# fill in missing brand codes with imputed values
df3$brand_code <- ifelse(!is.na(df3$brand_code), df3$brand_code, df3$brand2)
# remove brand prediction field
df3$brand2 <- NULL
set.seed(101)
trainIndex <-  createDataPartition(df3$ph,
p = 0.8,
list = F)
train <- df3[trainIndex, ]
test <- df3[-trainIndex, ]
# # MARS
# marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
# set.seed(101)
# mars <- train(ph ~ .,
#               data = train,
#               method = 'earth',
#               tuneGrid = marsGrid,
#               trControl = trainControl(method = 'cv')
#               )
# # SVM
# svm <- train(ph ~ .,
#              data = train,
#              method = 'svmRadial',
#              preProc = c('center', 'scale'),
#              tuneLength = 14,
#              trControl = trainControl(method = 'cv')
#              )
# # KNN
# set.seed(101)
# knn <- train(ph ~ .,
#              data = train,
#              method = 'knn',
#              preProc = c('center', 'scale'),
#              tuneGrid = data.frame(.k = 1:20),
#              trControl = trainControl(method = 'cv')
#              )
# # random forest
# rf <- randomForest(ph ~ .,
#                    data = train,
#                    ntrees = 1000)
# cubist
cubist <- train(ph ~ .,
data = train,
method = 'cubist')
# # predictions
# test$mars <- predict(mars, test)
# test$svm <- predict(svm, test)
# test$knn <- predict(knn, test)
# test$rf <- predict(rf, test)
# test$cubist <- predict(cubist, test)
#
# # measure of error and variance
# metrics <- data.frame(rbind(
#   postResample(pred = test$mars, obs = test$ph),
#   postResample(pred = test$svm, obs = test$ph),
#   postResample(pred = test$knn, obs = test$ph),
#   postResample(pred = test$rf, obs = test$ph),
#   postResample(pred = test$cubist, obs = test$ph)
# ),
#   row.names = c('MARS', 'SVM', 'KNN', 'Random Forest', 'Cubist')
# )
postResample(predict(cubist, test), test$ph)
knitr::opts_chunk$set(echo = TRUE, warning=F, message=F)
options(scipen = 9)
set.seed(101)
library(fpp2)
library(ggplot2)
library(tidyr)
library(dplyr)
library(stringr)
library(broom)
library(seasonal)
library(imputeTS)
library(tidymodels)
library(mice)
library(inspectdf)
library(vip)
library(lubridate)
library(corrplot)
library(caret)
library(fpp3)
library(randomForest)
library(Cubist)
library(knitr)
data_raw <- readr::read_csv('https://raw.githubusercontent.com/christianthieme/Predictive-Analytics/main/Project2/data/StudentData%20-%20TO%20MODEL.csv')
data_raw$obs_type <- "train"
eval_raw <- readr::read_csv('https://raw.githubusercontent.com/christianthieme/Predictive-Analytics/main/Project2/data/StudentEvaluation-%20TO%20PREDICT.csv')
eval_raw$obs_type <- "test"
combined <- data_raw %>%
rbind(eval_raw)
# convert column names to all lowercase
names(combined) <- lapply(names(combined), tolower)
# convert column name spaces to underscore
names(combined) <- str_replace_all(names(combined), ' ', '_')
df <- combined %>%
filter(obs_type == 'train') %>%
select(-obs_type)
eval <- combined %>%
filter(obs_type == 'test') %>%
select(-obs_type)
glimpse(combined)
df %>%
visdat::vis_miss(sort_miss = TRUE)
df %>%
select(ph) %>%
ggplot() +
aes(x = ph) +
geom_histogram(fill = "lightsalmon2", color = "black") +
labs(title = "Histogram of pH", y = "Count") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.45),
#   plot.margin = ggplot2::margin(10, 20, 10, 10),
panel.grid.major.y =  element_line(color = "grey", linetype = "dashed"),
panel.grid.major.x = element_blank(),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank()
)
df %>%
ggplot() +
aes(x = ph) +
geom_histogram(fill = "lightsalmon2", color = "black") +
labs(title = "Distribution of pH by Brand") +
facet_wrap(~brand_code, scales = 'free_y') +
theme(
plot.title = element_text(hjust = 0.45),
#   plot.margin = ggplot2::margin(10, 20, 10, 10),
panel.grid.major.y =  element_line(color = "grey", linetype = "dashed"),
panel.grid.major.x = element_blank(),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank()
)
df %>%
select(-ph) %>%
inspectdf::inspect_num() %>%
show_plot() +
labs(title = 'Distribution of Numeric Columns in Training Data')
df %>%
dplyr::select(brand_code, ph) %>%
ggplot() +
aes(y = ph) +
geom_boxplot(color = 'steelblue', outlier.color = 'firebrick', outlier.alpha = 0.35)+
labs(title = 'Brand Code Box Plots') +
facet_grid(~brand_code) +
#  theme_minimal() +
theme(
plot.title = element_text(hjust = 0.45),
panel.grid.major.y =  element_line(color = "grey", linetype = "dashed"),
panel.grid.major.x = element_blank(),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank(),
# panel.background = element_blank(),
axis.ticks.x = element_line(color = "grey")
)
#theme(axis.ticks.x = element_blank())
df %>%
select(-brand_code) %>%
gather(variable, value, -ph) %>%
ggplot(aes(x = value, y = ph)) +
geom_point(color = 'steelblue', alpha = 0.35) +
facet_wrap(~variable, scales = "free_x") +
labs(title = "pH Relationship with Independent Variables", x = element_blank())+
theme(
plot.title = element_text(hjust = 0.45),
panel.grid.major.y =  element_line(color = "grey", linetype = "dashed"),
panel.grid.major.x = element_blank(),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank(),
axis.ticks.x = element_line(color = "grey")
)
numeric_values <- df %>%
dplyr::select_if(is.numeric)
numeric_values <- numeric_values[complete.cases(numeric_values),] %>%
data.frame()
train_cor <- cor(numeric_values)
corrplot::corrplot.mixed(train_cor, tl.col = 'black', tl.pos = 'lt')
# remove rows without a response variable
df3 <- df %>%
filter(!is.na(ph)) %>%
mutate(brand_code = factor(brand_code))
# remove near zero variance variables
isNZV <- nearZeroVar(df3)
df3 <- df3[, -isNZV]
# imputing numeric variables
pp <- df3 %>%
as.data.frame() %>%
preProcess(method = c('knnImpute'))
# apply imputation of numeric variables
df3 <- predict(pp, df3)
# remove rows without a brand code
df3b <- df3 %>%
filter(!is.na(brand_code))
# predict class using multinomial logistic regression
pc <- train(brand_code ~ .,
data = df3b,
method = 'multinom',
trControl = trainControl(method = 'cv', number = 10),
trace = F)
# create new field with predictions of brand code
df3$brand2 <- predict(pc, df3)
# fill in missing brand codes with imputed values
df3$brand_code <- ifelse(!is.na(df3$brand_code), df3$brand_code, df3$brand2)
# remove brand prediction field
df3$brand2 <- NULL
set.seed(101)
trainIndex <-  createDataPartition(df3$ph,
p = 0.8,
list = F)
train <- df3[trainIndex, ]
test <- df3[-trainIndex, ]
# # MARS
# marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
# set.seed(101)
# mars <- train(ph ~ .,
#               data = train,
#               method = 'earth',
#               tuneGrid = marsGrid,
#               trControl = trainControl(method = 'cv')
#               )
# # SVM
# svm <- train(ph ~ .,
#              data = train,
#              method = 'svmRadial',
#              preProc = c('center', 'scale'),
#              tuneLength = 14,
#              trControl = trainControl(method = 'cv')
#              )
# # KNN
# set.seed(101)
# knn <- train(ph ~ .,
#              data = train,
#              method = 'knn',
#              preProc = c('center', 'scale'),
#              tuneGrid = data.frame(.k = 1:20),
#              trControl = trainControl(method = 'cv')
#              )
# # random forest
# rf <- randomForest(ph ~ .,
#                    data = train,
#                    ntrees = 1000)
# cubist
cubist <- train(ph ~ .,
data = train,
method = 'cubist')
# # predictions
# test$mars <- predict(mars, test)
# test$svm <- predict(svm, test)
# test$knn <- predict(knn, test)
# test$rf <- predict(rf, test)
# test$cubist <- predict(cubist, test)
#
# # measure of error and variance
# metrics <- data.frame(rbind(
#   postResample(pred = test$mars, obs = test$ph),
#   postResample(pred = test$svm, obs = test$ph),
#   postResample(pred = test$knn, obs = test$ph),
#   postResample(pred = test$rf, obs = test$ph),
#   postResample(pred = test$cubist, obs = test$ph)
# ),
#   row.names = c('MARS', 'SVM', 'KNN', 'Random Forest', 'Cubist')
# )
postResample(predict(cubist, test), test$ph)
plot(caret::varImp(cubist), main = "Cubist Regression Model Variable Importance")
# get all data excluding ph
eval2 <- combined %>%
select(-ph)
# convert brand code to factor
eval2$brand_code <- as.factor(eval2$brand_code)
# pre process evaluation data
ppe <- eval2 %>%
as.data.frame() %>%
preProcess(method = 'knnImpute')
# apply imputation of numeric variables
eval2 <- predict(ppe, eval2)
# remove rows without a brand code
eval2b <- eval2 %>%
filter(!is.na(brand_code))
# predict class using multinomial logistic regression, excluding pH
pc2 <- train(brand_code ~ .,
data = eval2b,
method = 'multinom',
trControl = trainControl(method = 'cv', number = 10),
trace = F)
# create new field with predictions of brand code
eval2$brand2 <- predict(pc2, eval2)
# fill in missing brand codes with imputed values
eval2$brand_code <- ifelse(!is.na(eval2$brand_code), eval2$brand_code, eval2$brand2)
# remove brand prediction field
eval2$brand2 <- NULL
# isolate evaluation data only
eval2 <- eval2 %>%
filter(obs_type == 'test') %>%
select(-obs_type)
final_predictions <- predict(cubist, eval2)
head(final_predictions)
#write.csv(final_predictions, "C:/Users/chris/OneDrive/Master Of Data Science - CUNY/Summer 2021/Predictive_Analytics/Predictive-Analytics/Project2/final_predictions.csv")
