---
title: "Data624 Project 2"
author: "Group 4"
date: "`r Sys.Date()`"
output:
  html_document:
    font-family: Consolas
    highlight: tango
    number_sections: no
    theme: paper
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
    toc_depth: '4'
---

```{=html}
<style type="text/css">

code {
  font-family: "Consolas";
  font-size: 11px;
}

pre {
  font-family: "Consolas";
  font-size: 11px;
}

mark {
  background-color: whitesmoke;
  color: black;
}

</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=F, message=F)

options(scipen = 9)
set.seed(101)

library(fpp2)
library(ggplot2)
library(tidyr)
library(dplyr)
library(stringr)
library(broom)
library(seasonal)
library(imputeTS)
library(tidymodels)
library(mice)
library(inspectdf)
library(lubridate)
library(corrplot)
library(caret)
library(fpp3)
library(randomForest)
library(Cubist)
```

<font size="3">Group Members</font>

-   Subhalaxmi Rout
-   Kenan Sooklall
-   Devin Teran
-   Christian Thieme
-   Leo Yi

\pagebreak

## Introduction

We have been given a dataset from a beverage manufacturing company that consists of 2,571 rows of data and 33 columns. The dataset contains information on different beverages and their chemical composition. The goal of this analysis is to use the 32 predictive features to predict the *Potential for hydrogen* (pH), which is a measure of the acidity/alkalinity of the beverage. pH is the key KPI in this analysis. 

We'll begin by reading in the dataset and looking at each column's data type: 

```{r}
data_raw <- readr::read_csv('https://raw.githubusercontent.com/christianthieme/Predictive-Analytics/main/Project2/data/StudentData%20-%20TO%20MODEL.csv')

data_raw$obs_type <- "train"

eval_raw <- readr::read_csv('https://raw.githubusercontent.com/christianthieme/Predictive-Analytics/main/Project2/data/StudentEvaluation-%20TO%20PREDICT.csv')

eval_raw$obs_type <- "test"

combined <- data_raw %>%
  rbind(eval_raw)

# convert column names to all lowercase
names(combined) <- lapply(names(combined), tolower)

# convert column name spaces to underscore
names(combined) <- str_replace_all(names(combined), ' ', '_')

df <- combined %>%
  filter(obs_type == 'train') %>%
  select(-obs_type)

eval <- combined %>%
  filter(obs_type == 'test') %>%
  select(-obs_type)

glimpse(combined)
```
We see that all columns, with the exception of `brand`, are doubles and continuous. Excluding the response variable, this means that we have 1 categorical variable and 31 continuous variables to work with. 

## Exploratory Data Analysis

In the output above, we can see that there are missing values (NAs). Let's see how pervasive this issue is within our dataset: 

```{r fig.width = 12, fig.height=6}
df %>%
  visdat::vis_miss(sort_miss = TRUE)
```

In total, only about 1% of our data is missing. We can see that most of the columns are only missing a negligible amount of data. `mfr` and `brand code` have the largest amount of missing values and are missing 8.25% and 4.67% of their data, respectively. Additionally, there does not appear to be a pattern in which values are missing. Now that we understand that our missing values are not a pervasive issue, we'll continue with our analysis.

#### Distribution of Response Variable: pH

Let's get an understanding of the distribution of our response variable: 

```{r}
df %>%
  select(ph) %>%
  ggplot() + 
  aes(x = ph) + 
  geom_histogram()
```

The distribution of pH is left-skewed and multi-modal. Generally speaking, when we see a multi-modal distribution, often times that is an indication that there are sub-populations within the distribution. We know from looking at our dataset that there is a `brand code` with values A, B, C, and D. Let's break up the above distribution into 4 distributions based on these values: 

```{r fig.height=6, fig.width=10}
df %>%
  ggplot() + 
  aes(x = ph) + 
  geom_histogram() +
  labs(title = "Distribution of pH by Brand") + 
  facet_wrap(~brand_code, scales = 'free_y')
```

Breaking down to this further grain does not seem to be much more helpful. There may be even more granular sub-populations within this data that we are not seeing. 

Now we'll turn our attention to the numeric features within our dataset: 

```{r fig.height=15, fig.width=10}
df %>%
  select(-ph) %>%
  inspectdf::inspect_num() %>%
  show_plot() +
  labs(title = 'Distribution of Numeric Columns in Training Data')
```

We note the following about these distributions: 

* `air_pressurer` - there appears to be either two distributions here, or a single distribution with a pocket of outliers
* `balling`, `balling_lvl`, `density`,`fill_ _pressure`, `hyd_pressure1`, `hyd_pressure2`, `hyd_pressure3`, `hyd_pressure4`, `mnf_flow`, `pressure_setpoint`- there appears to be two distributions here. This could potentially be connected to the type of `brand_code` or something else not as easily distinguishable. 
* `bowl_setpoint` - half of all the values are around 120
* `carb_flow` - most values fall between 3,000 and 4,000 with a large pocket of values at 1,000 as well
* `filler_speed`, `mfr`, `oxygen_filler` - either appears to have two distributions or a few significant outliers
* general note: it appears that many of these distributions are skewed in one way or another. We note that a transformation may be helpful when generating predictions. 

#### Explanatory Variable Relationships with the Response Variable

Now that we've looked at our response variable, let's look at our explanatory variables. We'll begin first by looking at `brand code`, which is our only categorical variable: 

```{r fig.height=5, fig.width=12}
df %>%
  select(brand_code, ph) %>%
  ggplot() + 
  aes(y = ph) + 
  geom_boxplot()+
  labs(title = 'Brand Code Box Plots') + 
  facet_grid(~brand_code) +
  theme(axis.ticks.x = element_blank())
```

We can see from the above boxplots that `brand_code` does have a meaningful relationship with pH. We can also see some significant outliers in C and possibly D that that will need to be evaluated further. We'll now turn our attention to the numeric features in our dataset.

**Numeric Features**

```{r fig.height=15, fig.width=12, message=FALSE, warning=FALSE}
df %>%
  select(-brand_code) %>% 
  gather(variable, value, -ph) %>%
  ggplot(aes(x = value, y = ph)) +
  geom_point() +
  facet_wrap(~variable, scales = "free_x") +
  labs(x = element_blank())
```

We note the following about the relationship between pH and these variables: 

* `air_pressurer` - it appears that there are two sub-groups here. In looking to see if this was due to `brand_code` we found that these sub-groups exist even at individual group levels
* `alch_rel` - most points appear to be gathered in 3 distinct areas, however there do appear to be 7 outliers
* `balling`, `balling_lvl` - it appears that there are two sub-groups here. These sub-groups do potentially look to be associated with `brand_code`
* `bowl_setpoint` - appears to potentially be a categorical variable as the values don't appear to be continuous. Also appear to potentially be some outliers
* `carb_flow` - appear to be two or three groups of data points. Also note the presence of outliers
* `density` - appear to be two to three groups of points. We note the presence of an outlier
* `filler_speed` - appear to mostly fall within the low or high range. Values in the middle are less frequent.
* `pressure_setpoint` - appear to be discrete values with the exception of 4 outliers
* `pressure_vacuum` - appear to be discrete values with a potentially positive linear relationship. We note the presence of an outlier
* `psc_co2` - appear to be discrete values. We note the presence of an outlier
* `psc_fill` - there appear to be 5 bands that values can fall into with certain areas that do not have values. We may consider adding a categorical variable to capture this
* `carb_pressure`, `carb_pressure1`, `carb_rel`, `carb_volume`, `fill_ounces`, `fill_pressure`, `filler_level`, `hyd_pressure1-4`, `mfr`, `oxygen_filler`, `pc_volume`, `psc`, `temperature`, `usage_cont`- no visible relationship. We do note the presence of outliers
* General note: It appears that many of these variables are on different scales. We'll take care of this during our data prep phase. 

**Correlated Features**

For many models, correlation between features can be an issue. Let's see what the correlation between our variables looks like: 

```{r fig.height=12, fig.width=12}
numeric_values <- df %>% 
  dplyr::select_if(is.numeric)

numeric_values <- numeric_values[complete.cases(numeric_values),] %>% 
  data.frame()

train_cor <- cor(numeric_values)

corrplot::corrplot.mixed(train_cor, tl.col = 'black', tl.pos = 'lt')
```

We see many of our features are *highly* correlated. There are several methods we could use to solve this, however, because we have many features, it may make sense to use principal component analysis, which will allow us to reduce the number of columns in our model and hopefully produce a simpler model. 

**Summary EDA Notes**
* Feature distributions are skewed and may benefit from a transformation
* Missing data will most likely not be a significant issue
* There are several outliers in our features - we should think about using a modeling technique that is robust against outliers
* Many of our features are significantly correlated with each other. PCA or another method may be helpful in reducing collinearity
* There appear to be sub-populations even within `brand_codes`. It may be helpful to do some feature engineering to tease this information from the data


## Data Processing

#### Data Imputation

We'll need to impute missing data for both numeric and categorical variables. We'll use the `knnImpute` method for the numeric variables and a multinomial logistic regression model to impute the `brand_code` variable. 

We do not use the combined dataset to train our model or impute values, under the assumption that this evaluation data is wouldn't be available to us when training our models. In practice, when we're faced with a new dataset, it would be impossible to use that data in the process of training our models.

```{r}
# remove rows without a response variable
df3 <- df %>%
  filter(!is.na(ph)) %>%
  mutate(brand_code = factor(brand_code))

# remove near zero variance variables
isNZV <- nearZeroVar(df3)
df3 <- df3[, -isNZV]

# imputing numeric variables
pp <- df3 %>%
  as.data.frame() %>%
  preProcess(method = 'knnImpute')

# apply imputation of numeric variables
df3 <- predict(pp, df3)

# remove rows without a brand code
df3b <- df3 %>%
  filter(!is.na(brand_code))

# predict class using multinomial logistic regression
pc <- train(brand_code ~ .,
            data = df3b,
            method = 'multinom',
            trControl = trainControl(method = 'cv', number = 10),
            trace = F)

# create new field with predictions of brand code
df3$brand2 <- predict(pc, df3)

# fill in missing brand codes with imputed values
df3$brand_code <- ifelse(!is.na(df3$brand_code), df3$brand_code, df3$brand2)

# remove brand prediction field
df3$brand2 <- NULL
```

#### Train Test Split

After imputing, we'll be using caret's `createDataPartition` to determine a stratified 80/20 split. Initially, we will use this one test dataset to evaluate all our models, however if two or more models have close error and variance measures, we can continue with cross validation on additional splits.

```{r}
set.seed(101)
trainIndex <-  createDataPartition(df3$ph,
                                   p = 0.8,
                                   list = F)

train <- df3[trainIndex, ]
test <- df3[-trainIndex, ]
```

## Modeling

We continue by fitting and tuning multiple models below. The goal is to see which one predicts with the lowest error and least variance on the test dataset.

```{r}
# MARS
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)

set.seed(101)
mars <- train(ph ~ .,
              data = train,
              method = 'earth',
              tuneGrid = marsGrid,
              trControl = trainControl(method = 'cv')
              )

mars$finalmodel

# SVM
svm <- train(ph ~ .,
             data = train,
             method = 'svmRadial',
             preProc = c('center', 'scale'),
             tuneLength = 14,
             trControl = trainControl(method = 'cv')
             )

svm$finalmodel

# KNN
set.seed(101)
knn <- train(ph ~ .,
             data = train,
             method = 'knn',
             preProc = c('center', 'scale'),
             tuneGrid = data.frame(.k = 1:20),
             trControl = trainControl(method = 'cv')
             )

knn$finalmodel

# random forest
rf <- randomForest(ph ~ .,
                   data = train,
                   ntrees = 1000)

rf

# cubist
cubist <- train(ph ~ .,
                data = train,
                method = 'cubist')

cubist$finalmodel
```

#### Compare Models

In order to evaluate the predictive power of each of the models, we'll use `postResample` to measure the error between each of the model's predictions and the holdout test data.

```{r}
# predictions
test$mars <- predict(mars, test)
test$svm <- predict(svm, test)
test$knn <- predict(knn, test)
test$rf <- predict(rf, test)
test$cubist <- predict(cubist, test)

# measure of error and variance
data.frame(rbind(
  postResample(pred = test$mars, obs = test$ph),
  postResample(pred = test$svm, obs = test$ph),
  postResample(pred = test$knn, obs = test$ph),
  postResample(pred = test$rf, obs = test$ph),
  postResample(pred = test$cubist, obs = test$ph)
),
  row.names = c('MARS', 'SVM', 'KNN', 'Random Forest', 'Cubist')
)
```

#### Variable Importance

Here, we'll take the best performing models and make note of any similarities in the most important variables. If any of the top variables had shown collinearity with other variables, it's possible that the variable importance is misstated and split amongst the correlated variables.

## Wrapping Up

#### Imputing Evaluation Data

We also need to impute missing data on the final evaluation data. We'll need to train another multinomial logistic regression model for brand code, excluding pH, and use `knnImpute` again for the numeric variables. We'll be using the entire combined dataset to impute these values under the assumption that we have trained our models on existing data and can leverage that to help us get a more accurate insight into the true population distributions.

```{r}
# get all data excluding ph
eval2 <- combined %>%
  select(-ph)

# convert brand code to factor
eval2$brand_code <- as.factor(eval2$brand_code)

# pre process evaluation data
ppe <- eval2 %>%
  as.data.frame() %>%
  preProcess(method = 'knnImpute')

# apply imputation of numeric variables
eval2 <- predict(ppe, eval2)


# remove rows without a brand code
eval2b <- eval2 %>%
  filter(!is.na(brand_code))

# predict class using multinomial logistic regression, excluding pH
pc2 <- train(brand_code ~ .,
            data = eval2b,
            method = 'multinom',
            trControl = trainControl(method = 'cv', number = 10),
            trace = F)

# create new field with predictions of brand code
eval2$brand2 <- predict(pc2, eval2)

# fill in missing brand codes with imputed values
eval2$brand_code <- ifelse(!is.na(eval2$brand_code), eval2$brand_code, eval2$brand2)

# remove brand prediction field
eval2$brand2 <- NULL

# isolate evaluation data only
eval2 <- eval2 %>%
  filter(obs_type == 'test') %>%
  select(-obs_type)
```










